--- /home/sgr/.pyenv/versions/general/lib/python3.6/site-packages/torch/nn/modules/activation.py
+++ /home/sgr/.pyenv/versions/general/lib/python3.6/site-packages/torch/nn/modules/activation.py
@@ -1,20 +1,14 @@
 class Softmax(Module):
     r"""Applies the Softmax function to an n-dimensional input Tensor
     rescaling them so that the elements of the n-dimensional output Tensor
-    lie in the range [0,1] and sum to 1.
+    lie in the range (0,1) and sum to 1
 
-    Softmax is defined as:
-
-    .. math::
-        \text{Softmax}(x_{i}) = \frac{\exp(x_i)}{\sum_j \exp(x_j)}
-
-    When the input Tensor is a sparse tensor then the unspecifed
-    values are treated as ``-inf``.
+    Softmax is defined as
+    :math:`\text{Softmax}(x_{i}) = \frac{\exp(x_i)}{\sum_j \exp(x_j)}`
 
     Shape:
-        - Input: :math:`(*)` where `*` means, any number of additional
-          dimensions
-        - Output: :math:`(*)`, same shape as the input
+        - Input: any shape
+        - Output: same as input
 
     Returns:
         a Tensor of the same dimension and shape as the input with
@@ -31,15 +25,12 @@
 
     Examples::
 
-        >>> m = nn.Softmax(dim=1)
+        >>> m = nn.Softmax()
         >>> input = torch.randn(2, 3)
         >>> output = m(input)
+    """
 
-    """
-    __constants__ = ['dim']
-    dim: Optional[int]
-
-    def __init__(self, dim: Optional[int] = None) -> None:
+    def __init__(self, dim=None):
         super(Softmax, self).__init__()
         self.dim = dim
 
@@ -48,9 +39,6 @@
         if not hasattr(self, 'dim'):
             self.dim = None
 
-    def forward(self, input: Tensor) -> Tensor:
+    def forward(self, input):
         return F.softmax(input, self.dim, _stacklevel=5)
 
-    def extra_repr(self) -> str:
-        return 'dim={dim}'.format(dim=self.dim)
-